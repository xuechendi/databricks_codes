{# sklearn_base.jinja is the base template that can be inherited from to generate model specific templates #}
{% import 'macros.jinja' as m with context %}

{# Redefine the block into a macro so that we can use it in more than one locations #}
{% macro model_definition() %}{% block model_definition %}{% endblock %}{% endmacro %}

{# define internal variables with prefix for ease of use #}
{% set var_classifier = m.with_prefix('classifier') %}
{% set var_training_metrics = m.with_prefix('training_metrics') %}
{% set var_val_metrics = m.with_prefix('val_metrics') %}
{% set var_test_metrics = m.with_prefix('test_metrics') %}
{% set var_sample_weight = m.with_prefix('sample_weight') %}

import mlflow
{% block imports %}{% endblock %}
import sklearn
from sklearn import set_config
from sklearn.pipeline import Pipeline
{% if need_target_encoder|default(false) %}
from sklearn.preprocessing import LabelEncoder
from databricks.automl_runtime.sklearn import TransformedTargetClassifier
{% endif %}

set_config(display="diagram")

{{ var_classifier }} = {{ model_definition() }}
{{ var_model }} = Pipeline([
    ("column_selector", {{ var_column_selector }}),
{% if var_preprocessor != None %}
    ("{{ var_preprocessor }}", {{ var_preprocessor }}),
{% endif %}
    ("classifier", {{ var_classifier }}),
])
{% if use_eval_set|default(false) %}
{% set var_pipeline_val = var_pipeline + "_val" %}
{% set var_X_val_processed = var_X_val + "_processed" %}

# Create a separate pipeline to transform the validation dataset. This is used for early stopping.
mlflow.sklearn.autolog(disable=True)
{{ var_pipeline_val }} = Pipeline([
    ("column_selector", {{ var_column_selector }}),
{% if var_preprocessor != None %}
    ("{{ var_preprocessor }}", {{ var_preprocessor }}),
{% endif %}
])
{{ var_pipeline_val }}.fit({{ var_X_train }}, {{ var_y_train }})
{{ var_X_val_processed }} = {{ var_pipeline_val }}.transform({{ var_X_val }})
{% if need_target_encoder|default(false) %}
{% set var_y_val_processed = var_y_val + "_processed" %}
{% set var_label_encoder_val = "label_encoder_val" %}
{{ var_label_encoder_val }} = LabelEncoder()
{{ var_label_encoder_val }}.fit({{ var_y_train }})
{{ var_y_val_processed }} = {{ var_label_encoder_val }}.transform({{ var_y_val }})
{% endif %}
{% endif %}

{{ var_model }}

# COMMAND ----------

{% set fit_params = [] %}
{% set _ =  fit_params.append(var_X_train) %}
{% set _ =  fit_params.append(var_y_train) %}

{% if use_eval_set|default(false) %}
  {% if model_name == 'lightgbm' %}
    {% set _ =  fit_params.append('classifier__callbacks=[lightgbm.early_stopping(5), lightgbm.log_evaluation(0)]') %}
  {% else %}
    {% set _ =  fit_params.append('classifier__early_stopping_rounds=5') %}
    {% set _ =  fit_params.append('classifier__verbose=False') %}
  {% endif %}
  {% if need_target_encoder|default(false) %}
    {% set _ =  fit_params.append('classifier__eval_set='+'[('+var_X_val_processed+','+var_y_val_processed+')]') %}
  {% else %}
    {% set _ =  fit_params.append('classifier__eval_set='+'[('+var_X_val_processed+','+var_y_val+')]') %}
  {% endif %}
{% endif %}

# Enable automatic logging of input samples, metrics, parameters, and models
mlflow.sklearn.autolog(log_input_examples=True, silent=True)

with mlflow.start_run(experiment_id="{{ experiment_id }}", run_name="{{ model_name | camelcase }}") as {{ var_run }}:
{% if sample_weight_col|default(false) %}
{% set _ = fit_params.append('classifier__sample_weight='+var_sample_weight) %}
{# Unlike pos_label_flag, sample_weight_flag is set here because it contains the variable var_sample_weight which is set in the template #}
{% set sample_weight_flag = ', sample_weight='+var_sample_weight %}
    # AutoML balanced the data internally and use {{ sample_weight_col }} to calibrate the probability distribution
    {{ var_sample_weight }} = {{ var_X_train }}.loc[:, "{{ sample_weight_col }}"].to_numpy()

{% endif %}
    {{ var_model }}.fit({{ fit_params|join(", ") }})
    {#
    - Most logged metrics use functions from sklearn.metrics, and are logged in mlflow repo's mlflow/sklearn/utils.py _get_classifier_metrics
    - training_score is computed by directly calling the model's score function in mlflow repo's mlflow/sklearn/__init__.py _log_posttraining_metadata
    However, based on sklearn's source code, it just uses sklearn.metrics.accuracy_score: https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L475
    - We need to override the training metrics logged by autologging if we use pos_label for other metrics or use sample_weight
    #}

    # Log metrics for the training set
    {{ var_training_metrics }} = mlflow.sklearn.eval_and_log_metrics({{ var_model }}, {{ var_X_train }}, {{ var_y_train }}, prefix="training_"{{ pos_label_flag|default("") }}{{ sample_weight_flag|default("") }})

    # Log metrics for the validation set
    {{ var_val_metrics }} = mlflow.sklearn.eval_and_log_metrics({{ var_model }}, {{ var_X_val }}, {{ var_y_val }}, prefix="val_"{{ pos_label_flag|default("") }})

    # Log metrics for the test set
    {{ var_test_metrics }} = mlflow.sklearn.eval_and_log_metrics({{ var_model }}, {{ var_X_test }}, {{ var_y_test }}, prefix="test_"{{ pos_label_flag|default("") }})

    # Display the logged metrics
    {{ var_val_metrics }} = {k.replace("val_", ""): v for k, v in {{ var_val_metrics }}.items()}
    {{ var_test_metrics }} = {k.replace("test_", ""): v for k, v in {{ var_test_metrics }}.items()}
    display(pd.DataFrame([{{ var_val_metrics }}, {{ var_test_metrics }}], index=["validation", "test"]))
